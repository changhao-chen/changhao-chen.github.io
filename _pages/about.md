---
layout: about
title: about
permalink: /
subtitle: <a href='https://www.hkust-gz.edu.cn/'>The Hong Kong University of Science and Technology</a>, Guangzhou, China

profile:
  align: right
  image: changhao.jpg
  image_circular: false # crops the image to make it circular
  more_info: >

selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page

announcements:
  enabled: true # includes a list of news items
  scrollable: true # adds a vertical scroll bar if there are more than 3 news items
  limit: 5 # leave blank to include all the news in the `_news` folder

latest_posts:
  enabled: false
  scrollable: false # adds a vertical scroll bar if there are more than 3 new posts items
  limit: 3 # leave blank to include all the blog posts
---

I am an Assistant Professor, at the <a href="https://www.hkust-gz.edu.cn/academics/hubs-and-thrust-areas/systems-hub/intelligent-transportation/">Intelligent Transportation Thrust</a> and <a href="https://www.hkust-gz.edu.cn/academics/hubs-and-thrust-areas/information-hub/artificial-intelligence/">Artificial Intelligence Thrust</a>, <a href="https://www.hkust-gz.edu.cn/">The Hong Kong University of Science and Technology (Guangzhou)</a>, China. Before that, I was a lecturer (2021-2024) at National University of Defense Technology, China, and a postdoctoral researcher (2020) at <a href="http://www.cs.ox.ac.uk/">Department of Computer Science</a>, <a href="http://www.ox.ac.uk/">University of Oxford</a>. I received Ph.D. in Computer Science (2016-2020) from <a href="http://www.ox.ac.uk/">University of Oxford</a>, supevised by <a href="http://www.cs.ox.ac.uk/people/niki.trigoni/">Prof. Niki Trigoni</a> and <a href="http://www.cs.ox.ac.uk/people/andrew.markham/">Prof. Andrew Markham</a>, Master in Engineering (2014-2016) from National University of Defense Technology, and Bachelor in Engineering (2010-2014) from the <a href="https://en.tongji.edu.cn/">Tongji University</a>, China.

I lead the HKUST-GZ <strong>PEAK Lab</strong> (<strong>P</strong>erception, <strong>E</strong>mbodiment, <strong>A</strong>utonomy and <strong>K</strong>inematics), where our research focuses on <strong><em>Embodied AI and Autonomous Systems</em></strong>, particularly the challenges of <strong><em>Open-World Robotic Perception, Navigation and Interaction</em></strong>.
Traditional robotic algorithms often depend on meticulously crafted geometric and dynamic models, which may struggle to adapt to ever-changing, complex environments.
Our research demonstrates that developing learning solutions over these static models enables autonomous systems to achieve independent motion estimation, robust spatial scene perception, and reliable, safe navigation. Our work involves a combination of novel algorithms and methods (including learning and statistics, signal processing, optimization, geometry, and dynamics modelling) and system implementations (including sensor fusion, hardware-software codesign, computing architecture).
Our research outcomes have been successfully applied to a diverse range of platforms, from robots, drones, self-driving vehicles to smartphone, smartwatches, and VR/AR devices, supporting their real-world applications in intelligent transportation, emergency rescue and hospital efficiency enhancement.

Our major contributions have been in the following research directions:

<strong><em>Ubiquitous Motion Tracking</em></strong>: we pioneer deep learning-based inertial positioning models that provide precise motion estimation and positioning in GPS-denied environments, regardless of environmental influences, including IONet, M2EIT, and MotionTransformer.

<strong><em>Robust Spatial Perception</em></strong>: our research develops novel learning and geometric methods for self-supervised learning based SLAM (e.g., SelfOdom, P2Net, DevNet), city-scale localization and rendering (e.g., AtLoc, DroneNeRF), and visually-degraded perception (e.g., Milli-Map, Milli-Ego, DarkSLAM, and ThermalLoc).

<strong><em>Safe Autonomous Navigation</em></strong>: our research develops task-driven multimodal fusion, stability-constrained dynamical modelling, and efficient policy learning, for safe robot navigation in the physical world, including SelectFusion, DynaNet and SGuidance.

<strong><em>Embodied AI</em></strong>: our recent research focuses on efficient foundation model for robot navigation policy learning (e.g. DynaNav), and LLM based global planning.
